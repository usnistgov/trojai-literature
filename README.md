
# TrojAI Literature Review 

The list below contains curated papers and arXiv articles that are related to Trojan attacks, backdoor attacks, and data poisoning on neural networks and machine learning systems. They are ordered from most to least recent. These articles were identified using variety of methods including:

- A [flair](https://github.com/flairNLP/flair) embedding created from the arXiv CS subset; details will be provided later
- A trained [ASReview](https://asreview.readthedocs.io/en/latest/) random forest model
- A curated manual literature review

1. [The TrojAI Software Framework: An OpenSource tool for Embedding Trojans into Deep Learning Models](arxiv.org/abs/2003.07233)
1. [BadNL: Backdoor Attacks Against NLP Models](arxiv.org/abs/2006.01043)
1. [Neural Network Calculator for Designing Trojan Detectors](https://arxiv.org/abs/2006.03707)
1. [Dynamic Backdoor Attacks Against Machine Learning Models](arxiv.org/abs/2003.03675)
1. [Vulnerabilities of Connectionist AI Applications: Evaluation and Defence](arxiv.org/abs/2003.08837)
1. [Backdoor Attacks on Federated Meta-Learning](arxiv.org/abs/2006.07026)
1. [Defending Support Vector Machines against Poisoning Attacks: the Hardness and Algorithm](arxiv.org/abs/2006.07757)
1. [Backdoors in Neural Models of Source Code](arxiv.org/abs/2006.06841)
1. [A new measure for overfitting and its implications for backdooring of deep learning](arxiv.org/abs/2006.06721)
1. [An Embarrassingly Simple Approach for Trojan Attack in Deep Neural Networks](arxiv.org/abs/2006.08131)
1. [MetaPoison: Practical General-purpose Clean-label Data Poisoning](arxiv.org/abs/2004.00225)
1. [Backdooring and Poisoning Neural Networks with Image-Scaling Attacks](arxiv.org/abs/2003.08633)
1. [Bullseye Polytope: A Scalable Clean-Label Poisoning Attack with Improved Transferability](arxiv.org/abs/2005.00191)
1. [On the Effectiveness of Mitigating Data Poisoning Attacks with Gradient Shaping](arxiv.org/abs/2002.11497)
1. [A Survey on Neural Trojans](https://eprint.iacr.org/2020/201.pdf)
1. [STRIP: A Defence Against Trojan Attacks on Deep Neural Networks](arxiv.org/abs/1902.06531)
1. [TrojDRL: Trojan Attacks on Deep Reinforcement Learning Agents](arxiv.org/abs/1903.06638)
1. [Regula Sub-rosa: Latent Backdoor Attacks on Deep Neural Networks](arxiv.org/abs/1905.10447)
1. [Februus: Input Purification Defense Against Trojan Attacks on Deep Neural Network Systems](arxiv.org/abs/1908.03369)
1. [TBT: Targeted Neural Network Attack with Bit Trojan](arxiv.org/abs/1909.05193)
1. [Bypassing Backdoor Detection Algorithms in Deep Learning](arxiv.org/abs/1905.13409)
1. [A backdoor attack against LSTM-based text classification systems](arxiv.org/abs/1905.12457)
1. [Invisible Backdoor Attacks Against Deep Neural Networks](arxiv.org/abs/1909.02742)
1. [Detecting AI Trojans Using Meta Neural Analysis](arxiv.org/abs/1910.03137)
1. [Label-Consistent Backdoor Attacks](arxiv.org/abs/1912.02771)
1. [NeuronInspect: Detecting Backdoors in Neural Networks via Output Explanations](arxiv.org/abs/1911.07399)
1. [Universal Litmus Patterns: Revealing Backdoor Attacks in CNNs](arxiv.org/abs/1906.10842)
1. [Programmable Neural Network Trojan for Pre-Trained Feature Extractor](arxiv.org/abs/1901.07766)
1. [Demon in the Variant: Statistical Analysis of DNNs for Robust Backdoor Contamination Detection](arxiv.org/abs/1908.00686)
1. [TamperNN: Efficient Tampering Detection of Deployed Neural Nets](arxiv.org/abs/1903.00317)
1. [TABOR: A Highly Accurate Approach to Inspecting and Restoring Trojan Backdoors in AI Systems](arxiv.org/abs/1908.01763)
1. [Design of intentional backdoors in sequential models](arxiv.org/abs/1902.09972)
1. [Design and Evaluation of a Multi-Domain Trojan Detection Method on Deep Neural Networks](arxiv.org/abs/1911.10312)
1. [Poison as a Cure: Detecting &amp; Neutralizing Variable-Sized Backdoor Attacks in Deep Neural Networks](arxiv.org/abs/1911.08040)
1. [Data Poisoning Attacks on Stochastic Bandits](arxiv.org/abs/1905.06494)
1. [Hidden Trigger Backdoor Attacks](arxiv.org/abs/1910.00033)
1. [Deep Poisoning Functions: Towards Robust Privacy-safe Image Data Sharing](arxiv.org/abs/1912.06895)
1. [A new Backdoor Attack in CNNs by training set corruption without label poisoning](arxiv.org/abs/1902.11237)
1. [Deep k-NN Defense against Clean-label Data Poisoning Attacks](arxiv.org/abs/1909.13374)
1. [Transferable Clean-Label Poisoning Attacks on Deep Neural Nets](arxiv.org/abs/1905.05897)
1. [Revealing Backdoors, Post-Training, in DNN Classifiers via Novel Inference on Optimized Perturbations Inducing Group Misclassification](arxiv.org/abs/1908.10498)
1. [Explaining Vulnerabilities to Adversarial Machine Learning through Visual Analytics](https://arxiv.org/abs/1907.07296)
1. [Subpopulation Data Poisoning Attacks](https://www.ccis.northeastern.edu/home/jagielski/subpop_finance.pdf)
1. [TensorClog: An imperceptible poisoning attack on deep neural network applications](https://ieeexplore.ieee.org/document/8668758)
1. [Deepinspect: A black-box trojan detection and mitigation framework for deep neural networks](https://cseweb.ucsd.edu/~jzhao/files/DeepInspect-IJCAI2019.pdf)
1. [Resilience of Pruned Neural Network Against Poisoning Attack](https://ieeexplore.ieee.org/document/8659362)
1. [Spectrum Data Poisoning with Adversarial Deep Learning](https://arxiv.org/abs/1901.09247)
1. [Neural cleanse: Identifying and mitigating backdoor attacks in neural networks](http://people.cs.uchicago.edu/~huiyingli/publication/backdoor-sp19.pdf)
1. [SentiNet: Detecting Localized Universal Attacks Against Deep Learning Systems](arxiv.org/abs/1812.00292)
1. [PoTrojan: powerful neural-level trojan designs in deep learning models](arxiv.org/abs/1802.03043)
1. [Hardware Trojan Attacks on Neural Networks](arxiv.org/abs/1806.05768)
1. [Spectral Signatures in Backdoor Attacks](arxiv.org/abs/1811.00636)
1. [Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering](arxiv.org/abs/1811.03728)
1. [Model-Reuse Attacks on Deep Learning Systems](arxiv.org/abs/1812.00483)
1. [How To Backdoor Federated Learning](arxiv.org/abs/1807.00459)
1. [Trojaning Attack on Neural Networks](https://docs.lib.purdue.edu/cgi/viewcontent.cgi?article=2782&context=cstech)
1. [Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks](arxiv.org/abs/1804.00792)
1. [Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks](arxiv.org/abs/1805.12185)
1. [Technical Report: When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks](arxiv.org/abs/1803.06975)
1. [Backdoor Embedding in Convolutional Neural Network Models via Invisible Perturbation](arxiv.org/abs/1808.10307)
1. [Hu-Fu: Hardware and Software Collaborative Attack Framework against Neural Networks](arxiv.org/abs/1805.05098)
1. [Attack Strength vs](arxiv.org/abs/1802.07295)
1. [Data Poisoning Attacks in Contextual Bandits](arxiv.org/abs/1808.05760)
1. [BEBP: An Poisoning Method Against Machine Learning Based IDSs](arxiv.org/abs/1803.03965)
1. [Generative Poisoning Attack Method Against Neural Networks](arxiv.org/abs/1703.01340)
1. [BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain](arxiv.org/abs/1708.06733)
1. [Towards Poisoning of Deep Learning Algorithms with Back-gradient Optimization](arxiv.org/abs/1708.08689)
1. [Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning](arxiv.org/abs/1712.05526)
1. [Neural Trojans](arxiv.org/abs/1710.00942)
1. [Towards Poisoning of Deep Learning Algorithms with Back-gradient Optimization](arxiv.org/abs/1708.08689)
1. [Certified defenses for data poisoning attacks](https://arxiv.org/abs/1706.03691)
1. [Data Poisoning Attacks on Factorization-Based Collaborative Filtering](arxiv.org/abs/1608.08182)
1. [Data poisoning attacks against autoregressive models](https://dl.acm.org/doi/10.5555/3016100.3016102)
1. [Using machine teaching to identify optimal training-set attacks on machine learners](https://dl.acm.org/doi/10.5555/2886521.2886721)
1. [Poisoning Attacks against Support Vector Machines](arxiv.org/abs/1206.6389)
1. [Antidote: Understanding and defending against poisoning of anomaly detectors](https://dl.acm.org/doi/10.1145/1644893.1644895)
