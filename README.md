# TrojAI Literature Review 

The list below contains curated papers and arXiv articles that are related to Trojan attacks, backdoor attacks, and data poisoning on neural networks and machine learning systems. They are ordered "approximately" from most to least recent and articles denoted with a "*" mention the TrojAI program directly. Some of the particularly relevant papers include a summary that can be accessed by clicking the "Summary" drop down icon underneath the paper link. These articles were identified using variety of methods including:

- A [flair](https://github.com/flairNLP/flair) embedding created from the arXiv CS subset; details will be provided later.
- A trained [ASReview](https://asreview.readthedocs.io/en/latest/) random forest model
- A curated manual literature review

1. [Physical Adversarial Attack meets Computer Vision: A Decade Survey](https://web10.arxiv.org/pdf/2209.15179.pdf)
1. [Data Poisoning Attacks Against Multimodal Encoders](https://web10.arxiv.org/pdf/2209.15266.pdf)
1. [MARNet: Backdoor Attacks Against Cooperative Multi-Agent Reinforcement Learning](https://ieeexplore.ieee.org/abstract/document/9894692)
1. [Not All Poisons are Created Equal: Robust Training against Data Poisoning](https://proceedings.mlr.press/v162/yang22j/yang22j.pdf)
1. [Evil vs evil: using adversarial examples against backdoor attack in federated learning](https://link.springer.com/article/10.1007/s00530-022-00965-z)
1. [Auditing Visualizations: Transparency Methods Struggle to Detect Anomalous Behavior](https://arxiv.org/pdf/2206.13498.pdf)
1. [Defending Backdoor Attacks on Vision Transformer via Patch Processing](https://arxiv.org/pdf/2206.12381.pdf)
1. [Defense against backdoor attack in federated learning](https://www.sciencedirect.com/science/article/pii/S0167404822002139)
1. [SentMod: Hidden Backdoor Attack on Unstructured Textual Data](https://ieeexplore.ieee.org/abstract/document/9799471)
1. [Adversarial poisoning attacks on reinforcement learning-driven energy pricing](http://people.eecs.berkeley.edu/~orrp/files/gjpss.pdf)
1. [Natural Backdoor Datasets](https://arxiv.org/pdf/2206.10673.pdf)
1. [Backdoor Attacks and Defenses in Federated Learning: State-of-the-art, Taxonomy, and Future Directions](https://ieeexplore.ieee.org/abstract/document/9806416)
3. [VulnerGAN: a backdoor attack through vulnerability amplification against machine learning-based network intrusion detection systems](http://scis.scichina.com/en/2022/170303.pdf)
4. [Hiding Needles in a Haystack: Towards Constructing Neural Networks that Evade Verification](https://dl.acm.org/doi/abs/10.1145/3531536.3532966)
5. [TrojanZoo: Towards Unified, Holistic, and Practical Evaluation of Neural Backdoors](https://ieeexplore.ieee.org/abstract/document/9797345)
6. [Camouflaged Poisoning Attack on Graph Neural Networks](https://dl.acm.org/doi/abs/10.1145/3512527.3531373)
7. [BackdoorBench: A Comprehensive Benchmark of Backdoor Learning](https://arxiv.org/pdf/2206.12654.pdf)
8. [Fooling a Face Recognition System with a Marker-Free Label-Consistent Backdoor Attack](https://link.springer.com/chapter/10.1007/978-3-031-06430-2_15?noAccess=true)
9. [Backdoor Attacks on Bayesian Neural Networks using Reverse Distribution](https://arxiv.org/pdf/2205.09167.pdf)
10. [Design of AI Trojans for Evading Machine Learning-based Detection of Hardware Trojans](https://ieeexplore.ieee.org/abstract/document/9774654)
11. [PoisonedEncoder: Poisoning the Unlabeled Pre-training Data in Contrastive Learning](https://arxiv.org/pdf/2205.06401.pdf)
12. [Model-Contrastive Learning for Backdoor Defense](https://arxiv.org/pdf/2205.04411.pdf)
13. [Robust Anomaly based Attack Detection in Smart Grids under Data Poisoning Attacks](https://dl.acm.org/doi/abs/10.1145/3494107.3522778)
14. [Disguised as Privacy: Data Poisoning Attacks against Differentially Private Crowdsensing Systems](https://ieeexplore.ieee.org/abstract/document/9775757)
15. [Poisoning attack toward visual classification model](http://www.joca.cn/EN/10.11772/j.issn.1001-9081.2021122068)
16. [Verifying Neural Networks Against Backdoor Attacks](https://arxiv.org/pdf/2205.06992.pdf)
17. [VPN: Verification of Poisoning in Neural Networks](https://arxiv.org/pdf/2205.03894.pdf)
18. [LinkBreaker: Breaking the Backdoor-Trigger Link in DNNs via Neurons Consistency Check](https://ieeexplore.ieee.org/abstract/document/9775729) 
19. [A Study of the Attention Abnormality in Trojaned BERTs](https://arxiv.org/pdf/2205.08305.pdf)
20. [Universal Post-Training Backdoor Detection](https://arxiv.org/pdf/2205.06900.pdf)
21. [Planting Undetectable Backdoors in Machine Learning Models](https://arxiv.org/abs/2204.06974)
22. [Natural Backdoor Attacks on Deep Neural Networks via Raindrops](https://www.hindawi.com/journals/scn/2022/4593002/)
23. [MPAF: Model Poisoning Attacks to Federated Learning based on Fake Clients](https://arxiv.org/pdf/2203.08669.pdf)
24. [PiDAn: A Coherence Optimization Approach for Backdoor Attack Detection and Mitigation in Deep Neural Networks](https://arxiv.org/pdf/2203.09289.pdf)
25. [ADFL: A Poisoning Attack Defense Framework for Horizontal Federated Learning](https://ieeexplore.ieee.org/abstract/document/9735274)
26. [Toward Realistic Backdoor Injection Attacks on DNNs using Rowhammer](https://arxiv.org/pdf/2110.07683.pdf)
27. [Execute Order 66: Targeted Data Poisoning for Reinforcement Learning via Minuscule Perturbations](https://arxiv.org/pdf/2201.00762.pdf)
28. [A Feature Based On-Line Detector to Remove Adversarial-Backdoors by Iterative Demarcation](https://ieeexplore.ieee.org/document/9673744)
29. [BlindNet backdoor: Attack on deep neural network using blind watermark](https://link.springer.com/article/10.1007/s11042-021-11135-0)
30. [DBIA: Data-free Backdoor Injection Attack against Transformer Networks](https://arxiv.org/pdf/2111.11870.pdf)
31. [Backdoor Attack through Frequency Domain](https://arxiv.org/abs/2111.10991)
32. [NTD: Non-Transferability Enabled Backdoor Detection](https://arxiv.org/abs/2111.11157#:~:text=This%20work%20observes%20that%20all,implanted%20with%20the%20same%20backdoor.)
33. [Romoa: Robust Model Aggregation for the Resistance of Federated Learning to Model Poisoning Attacks](https://link.springer.com/chapter/10.1007/978-3-030-88418-5_23)
34. [Generative strategy based backdoor attacks to 3D point clouds: Work in Progress](https://dl.acm.org/doi/abs/10.1145/3477244.3477611)
35. [Deep Neural Backdoor in Semi-Supervised Learning: Threats and Countermeasures](https://ieeexplore.ieee.org/abstract/document/9551983)
36. [FooBaR: Fault Fooling Backdoor Attack on Neural Network Training](https://arxiv.org/pdf/2109.11249.pdf)
37. [BFClass: A Backdoor-free Text Classification Framework](https://arxiv.org/pdf/2109.10855.pdf)
38. [Backdoor Attacks on Federated Learning with Lottery Ticket Hypothesis](https://arxiv.org/pdf/2109.10512.pdf)
39. [Data Poisoning against Differentially-Private Learners: Attacks and Defenses](https://arxiv.org/pdf/1903.09860.pdf)
40. [DOES DIFFERENTIAL PRIVACY DEFEAT DATA POISONING?](https://dp-ml.github.io/2021-workshop-ICLR/files/23.pdf)
41. [Check Your Other Door! Establishing Backdoor Attacks in the Frequency Domain](https://arxiv.org/pdf/2109.05507.pdf)
42. [HaS-Nets: A Heal and Select Mechanism to Defend DNNs Against Backdoor Attacks for Data Collection Scenarios](https://arxiv.org/abs/2012.07474)
43. [SanitAIs: Unsupervised Data Augmentation to Sanitize Trojaned Neural Networks](https://arxiv.org/pdf/2109.04566.pdf)
44. [COVID-19 Diagnosis from Chest X-Ray Images Using Convolutional Neural Networks and Effects of Data Poisoning](https://link.springer.com/chapter/10.1007/978-3-030-87013-3_38)
45. [Interpretability-Guided Defense against Backdoor Attacks to Deep Neural Networks](https://ieeexplore.ieee.org/abstract/document/9530722)
46. [Trojan Signatures in DNN Weights](https://arxiv.org/pdf/2109.02836.pdf)
47. [HOW TO INJECT BACKDOORS WITH BETTER CONSISTENCY: LOGIT ANCHORING ON CLEAN DATA](https://arxiv.org/pdf/2109.01300.pdf)
48. [A Synergetic Attack against Neural Network Classifiers combining Backdoor and Adversarial Examples](https://arxiv.org/pdf/2109.01275.pdf)
49. [Backdoor Attack and Defense for Deep Regression](https://arxiv.org/pdf/2109.02381.pdf)
50. [Use Procedural Noise to Achieve Backdoor Attack](https://ieeexplore.ieee.org/abstract/document/9529206)
51. [Excess Capacity and Backdoor Poisoning](https://arxiv.org/pdf/2109.00685.pdf)
52. [BatFL: Backdoor Detection on Federated Learning in e-Health](https://ieeexplore.ieee.org/abstract/document/9521339)
53. [Poisonous Label Attack: Black-Box Data Poisoning Attack with Enhanced Conditional DCGAN](https://link.springer.com/article/10.1007/s11063-021-10584-w)
54. [Backdoor Attacks on Network Certification via Data Poisoning](https://arxiv.org/pdf/2108.11299.pdf)
55. [Identifying Physically Realizable Triggers for Backdoored Face Recognition Networks](https://ieeexplore.ieee.org/abstract/document/9506564)
56. [Simtrojan: Stealthy Backdoor Attack](https://ieeexplore.ieee.org/abstract/document/9506313)
57. [Back to the Drawing Board: A Critical Evaluation of Poisoning Attacks on Federated Learning](https://arxiv.org/pdf/2108.10241.pdf)
58. [Quantization Backdoors to Deep Learning Models](https://arxiv.org/pdf/2108.09187.pdf)
59. [Multi-Target Invisibly Trojaned Networks for Visual Recognition and Detection](https://www.ijcai.org/proceedings/2021/0477.pdf)
60. [A Countermeasure Method Using Poisonous Data Against Poisoning Attacks on IoT Machine Learning](https://www.worldscientific.com/doi/abs/10.1142/S1793351X21400043)
61. [FederatedReverse: A Detection and Defense Method Against Backdoor Attacks in Federated Learning](https://dl.acm.org/doi/abs/10.1145/3437880.3460403)
62. [Accumulative Poisoning Attacks on Real-time Data](https://arxiv.org/pdf/2106.09993.pdf)
63. [Inaudible Manipulation of Voice-Enabled Devices Through BackDoor Using Robust Adversarial Audio Attacks](https://dl.acm.org/doi/abs/10.1145/3468218.3469048)
64. [Stealthy Targeted Data Poisoning Attack on Knowledge Graphs](https://ieeexplore.ieee.org/abstract/document/9458733)
65. [BinarizedAttack: Structural Poisoning Attacks to Graph-based Anomaly Detection](https://arxiv.org/pdf/2106.09989.pdf)
66. [On the Effectiveness of Poisoning against Unsupervised Domain Adaptation](https://openreview.net/pdf?id=qArDlGXA9H)
67. [Simple, Attack-Agnostic Defense Against Targeted Training Set Attacks Using Cosine Similarity](http://www.gatsby.ucl.ac.uk/~balaji/udl2021/accepted-papers/UDL2021-paper-029.pdf)
68. [Data Poisoning Attacks Against Outcome Interpretations of Predictive Models](https://dl.acm.org/doi/abs/10.1145/3447548.3467405)
69. [BDDR: An Effective Defense Against Textual Backdoor Attacks](https://www.sciencedirect.com/science/article/pii/S0167404821002571)
70. [Poisoning attacks and countermeasures in intelligent networks: status quo and prospects](https://www.sciencedirect.com/science/article/pii/S235286482100050X)
71. [The Devil is in the GAN: Defending Deep Generative Models Against Backdoor Attacks](https://arxiv.org/pdf/2108.01644.pdf)
72. [BadEncoder: Backdoor Attacks to Pre-trainedEncoders in Self-Supervised Learning](https://arxiv.org/pdf/2108.00352.pdf)
73. [BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning](https://arxiv.org/pdf/2108.00352.pdf)
74. [Can You Hear It? Backdoor Attacks via Ultrasonic Triggers](https://arxiv.org/pdf/2107.14569.pdf)
75. [Poisoning Attacks via Generative Adversarial Text to Image Synthesis](https://ieeexplore.ieee.org/abstract/document/9502441)
76. [Ant Hole: Data Poisoning Attack Breaking out the Boundary of Face Cluster](https://ieeexplore.ieee.org/abstract/document/9502431)
77. [Poison Ink: Robust and Invisible Backdoor Attack](https://arxiv.org/pdf/2108.02488.pdf)
78. [MT-MTD: Muti-Training based Moving Target Defense Trojaning Attack in Edged-AI network](https://ieeexplore.ieee.org/abstract/document/9500545)
79. [Text Backdoor Detection Using An Interpretable RNN Abstract Model](https://ieeexplore.ieee.org/abstract/document/9508422)
80. [Garbage in, Garbage out: Poisoning Attacks Disguised with Plausible Mobility in Data Aggregation](https://ieeexplore.ieee.org/abstract/document/9511094)
81. [Classification Auto-Encoder based Detector against Diverse Data Poisoning Attacks](https://arxiv.org/pdf/2108.04206.pdf)
82. [Poisoning Knowledge Graph Embeddings via Relation Inference Patterns](https://aclanthology.org/2021.acl-long.147.pdf)
83. [Adversarial Training Time Attack Against Discriminative and Generative Convolutional Models](https://ieeexplore.ieee.org/abstract/document/9502126)
84. [Poisoning of Online Learning Filters: DDoS Attacks and Countermeasures](https://arxiv.org/abs/2107.12612)
85. [Rethinking Stealthiness of Backdoor Attack against NLP Models](https://aclanthology.org/2021.acl-long.431.pdf)
86. [Robust Learning for Data Poisoning Attacks](http://proceedings.mlr.press/v139/wang21r/wang21r.pdf)
87. [SPECTRE: Defending Against Backdoor Attacks Using Robust Statistics](http://proceedings.mlr.press/v139/hayase21a/hayase21a.pdf)
88. [Poisoning the Search Space in Neural Architecture Search](https://arxiv.org/pdf/2106.14406.pdf)
89. [Data Poisoning Won’t Save You From Facial Recognition](https://arxiv.org/pdf/2106.14851.pdf)
90. [Accumulative Poisoning Attacks on Real-time Data](https://arxiv.org/pdf/2106.09993.pdf)
91. [Backdoor Attack on Machine Learning Based Android Malware Detectors](https://www.computer.org/csdl/journal/tq/5555/01/09477038/1v2Mgglt5ew)
92. [Understanding the Limits of Unsupervised Domain Adaptation via Data Poisoning](https://arxiv.org/pdf/2107.03919.pdf)
93. [Indirect Invisible Poisoning Attacks on Domain Adaptation](http://publish.illinois.edu/junwu3/files/2021/06/KDD21_camera_ready_I2Attack-2.pdf)
94. [Fight Fire with Fire: Towards Robust Recommender Systems via Adversarial Poisoning Training](https://dl.acm.org/doi/abs/10.1145/3404835.3462914)
95. [Putting words into the system’s mouth: A targeted attack on neural machine translation using monolingual data poisoning](https://arxiv.org/pdf/2107.05243.pdf)
96. [SUBNET REPLACEMENT: DEPLOYMENT-STAGE BACKDOOR ATTACK AGAINST DEEP NEURAL NETWORKS IN GRAY-BOX SETTING](https://arxiv.org/abs/2107.07240)
97. [Spinning Sequence-to-Sequence Models with Meta-Backdoors](https://arxiv.org/abs/2107.10443)
98. [Sleeper Agent: Scalable Hidden Trigger Backdoors for Neural Networks Trained from Scratch](https://arxiv.org/abs/2106.08970)
99. [Poisoning and Backdooring Contrastive Learning](https://arxiv.org/abs/2106.09667)
100. [AdvDoor: Adversarial Backdoor Attack of Deep Learning System](http://www.wingtecher.com/themes/WingTecherResearch/assets/papers/issta21_learning.pdf)
101. [Defending against Backdoor Attacks in Natural Language Generation](https://arxiv.org/abs/2106.01810)
102. [De-Pois: An Attack-Agnostic Defense against Data Poisoning Attacks](https://arxiv.org/pdf/2105.03592.pdf)
103. [Poisoning MorphNet for Clean-Label Backdoor Attack to Point Clouds](https://arxiv.org/abs/2105.04839)
104. [Provable Guarantees against Data Poisoning Using Self-Expansion and Compatibility](https://arxiv.org/abs/2105.03692)
105. [MLDS: A Dataset for Weight-Space Analysis of Neural Networks](https://arxiv.org/pdf/2104.10555.pdf)
106. [Poisoning the Unlabeled Dataset of Semi-Supervised Learning](https://arxiv.org/abs/2105.01622)
107. [Regularization Can Help Mitigate Poisioning
Attacks. . . With The Right Hyperparameters](https://www.researchgate.net/profile/Javier-Carnerero-Cano/publication/351273823_Regularization_Can_Help_Mitigate_Poisoning_Attacks_with_the_Right_Hyperparameters/links/608ed4eb299bf1ad8d72886d/Regularization-Can-Help-Mitigate-Poisoning-Attacks-with-the-Right-Hyperparameters.pdf)
1. [Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching](https://arxiv.org/abs/2009.02276)
1. [Towards Robustness Against Natural Language Word Substitutions](https://openreview.net/forum?id=ks5nebunVn_)
1. [Concealed Data Poisoning Attacks on NLP Models](https://arxiv.org/abs/2010.12563)
1. [Covert Channel Attack to Federated Learning Systems](https://arxiv.org/abs/2104.10561)
1. [Backdoor Attacks Against Deep Learning Systems in the Physical World](https://arxiv.org/abs/2006.14580)
1. [Backdoor Attacks on Self-Supervised Learning](https://www.csee.umbc.edu/~hpirsiav/papers/Backdoor_SSL.pdf)
1. [Transferable Environment Poisoning: Training-time Attack on Reinforcement Learning](https://dl.acm.org/doi/abs/10.5555/3461017.3461172)
1. [Investigation of a differential cryptanalysis inspired approach for Trojan AI detection](https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11746/117460X/Investigation-of-a-differential-cryptanalysis-inspired-approach-for-Trojan-AI/10.1117/12.2588008.short?SSO=1)
1. [Explanation-Guided Backdoor Poisoning Attacks Against Malware Classifiers](https://arxiv.org/abs/2003.01031)
1. [Robust Backdoor Attacks against Deep Neural Networks in Real Physical World](https://arxiv.org/abs/2104.07395)
1. [The Design and Development of a Game to Study Backdoor Poisoning Attacks: The Backdoor Game](https://dl.acm.org/doi/abs/10.1145/3397481.3450647)
1. [A Backdoor Attack against 3D Point Cloud Classifiers](https://arxiv.org/abs/2104.05808)
1. [Explainability-based Backdoor Attacks Against Graph Neural Networks](https://arxiv.org/abs/2104.03674)
1. [DeepSweep: An Evaluation Framework for Mitigating DNN Backdoor Attacks using Data Augmentation](https://arxiv.org/abs/2012.07006)
1. [Rethinking the Backdoor Attacks' Triggers: A Frequency Perspective](https://arxiv.org/abs/2104.03413)
1. [PointBA: Towards Backdoor Attacks in 3D Point Cloud](https://arxiv.org/pdf/2103.16074.pdf)
1. [Online Defense of Trojaned Models using Misattributions](https://arxiv.org/abs/2103.15918)
2. [Be Careful about Poisoned Word Embeddings: Exploring the Vulnerability of the Embedding Layers in NLP Models](https://arxiv.org/abs/2103.15543)
3. [SPECTRE: Defending Against Backdoor Attacks Using Robust Covariance Estimation](https://homes.cs.washington.edu/~sewoong/backdoor.pdf)
4. [Black-box Detection of Backdoor Attacks with Limited Information and Data](https://arxiv.org/abs/2103.13127)
5. [TOP: Backdoor Detection in Neural Networks via Transferability of Perturbation](https://arxiv.org/abs/2103.10274)
6. [T-Miner : A Generative Approach to Defend Against Trojan Attacks on DNN-based Text Classification](https://arxiv.org/pdf/2103.04264.pdf)
7. [Hidden Backdoor Attack against Semantic Segmentation Models](https://arxiv.org/abs/2103.04038)
8. [What Doesn't Kill You Makes You Robust(er): Adversarial Training against Poisons and Backdoors](https://arxiv.org/abs/2102.13624)
9. [Red Alarm for Pre-trained Models: Universal Vulnerabilities by Neuron-Level Backdoor Attacks](https://arxiv.org/abs/2101.06969)
10. [Provable Defense Against Delusive Poisoning](https://arxiv.org/abs/2102.04716)
11. [An Approach for Poisoning Attacks Against RNN-Based Cyber Anomaly Detection](https://ieeexplore.ieee.org/abstract/document/9343232)
12. [Backdoor Scanning for Deep Neural Networks through K-Arm Optimization](https://arxiv.org/abs/2102.05123)
13. [TAD: Trigger Approximation based Black-box Trojan Detection for AI*](https://arxiv.org/abs/2102.01815)
14. [WaNet - Imperceptible Warping-based Backdoor Attack](https://openreview.net/pdf?id=eEn8KTtJOx)
15. [Data Poisoning Attack on Deep Neural Network and Some Defense Methods](https://conferences.computer.org/acomppub/pdfs/ACOMP2020-3FLOMB9Cka9w5SsnAX9Hs0/816700a015/816700a015.pdf)
16. [Baseline Pruning-Based Approach to Trojan Detection in Neural Networks*](https://arxiv.org/abs/2101.12016)
17. [Covert Model Poisoning Against Federated Learning: Algorithm Design and Optimization](https://arxiv.org/abs/2101.11799)
18. [Property Inference from Poisoning](https://arxiv.org/abs/2101.11073)
19. [TROJANZOO: Everything you ever wanted to know about neural backdoors (but were afraid to ask)](https://arxiv.org/abs/2012.09302)
20. [A Master Key Backdoor for Universal Impersonation Attack against DNN-based Face Verification](https://www.sciencedirect.com/science/article/abs/pii/S0167865521000210#!)
21. [Detecting Universal Trigger's Adversarial Attack with Honeypot](https://arxiv.org/abs/2011.10492) 
22. [ONION: A Simple and Effective Defense Against Textual Backdoor Attacks](https://arxiv.org/abs/2011.10369)
23. [Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks](https://arxiv.org/abs/2101.05930)
24. [Data Poisoning Attacks to Deep Learning Based Recommender Systems](https://arxiv.org/abs/2101.02644)
25. [Backdoors hidden in facial features: a novel invisible backdoor attack against face recognition systems](https://link.springer.com/article/10.1007/s12083-020-01031-z)
26. [One-to-N & N-to-One: Two Advanced Backdoor Attacks against Deep Learning Models](https://ieeexplore.ieee.org/document/9211729)
27. [DeepPoison: Feature Transfer Based Stealthy Poisoning Attack](https://arxiv.org/abs/2101.02562)
28. [Policy Teaching via Environment Poisoning:Training-time Adversarial Attacks against Reinforcement Learning](https://arxiv.org/pdf/2003.12909.pdf)
29. [Composite Backdoor Attack for Deep Neural Network by Mixing Existing Benign Features](https://dl.acm.org/doi/10.1145/3372297.3423362)
30. [SPA: Stealthy Poisoning Attack](https://dl.acm.org/doi/abs/10.1145/3444370.3444589)
31. [Backdoor Attack with Sample-Specific Triggers](https://arxiv.org/abs/2012.03816)
32. [Explainability Matters: Backdoor Attacks on Medical Imaging](http://arxiv.org/abs/2101.00008)
33. [Escaping Backdoor Attack Detection of Deep Learning](https://link.springer.com/chapter/10.1007/978-3-030-58201-2_29)
34. [Just How Toxic is Data Poisoning? A Unified Benchmark for Backdoor and Data Poisoning Attacks](https://arxiv.org/abs/2006.12557)
35. [Poisoning Attacks on Cyber Attack Detectors for Industrial Control Systems](https://arxiv.org/pdf/2012.15740.pdf)
36. [Fair Detection of Poisoning Attacks in Federated Learning](https://ieeexplore.ieee.org/abstract/9288279)
37. [Deep Feature Space Trojan Attack of Neural Networks by Controlled Detoxification*](https://arxiv.org/pdf/2012.11212.pdf)
38. [Stealthy Poisoning Attack on Certified Robustness](http://www.cs.tulane.edu/~jhamm3/papers/neuripsw20am.pdf)
39. [Machine Learning with Electronic Health Records is vulnerable to Backdoor Trigger Attacks](http://www.cs.tulane.edu/~jhamm3/papers/aaaiw21bj.pdf) 
40. [Data Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses](https://arxiv.org/pdf/2012.10544.pdf)
41. [Detection of Backdoors in Trained Classifiers Without Access to the Training Set](https://pubmed.ncbi.nlm.nih.gov/33326384/)
42. [TROJANZOO: Everything you ever wanted to know about neural backdoors(but were afraid to ask)](https://arxiv.org/pdf/2012.09302.pdf)
43. [HaS-Nets: A Heal and Select Mechanism to Defend DNNs Against Backdoor Attacks for Data Collection Scenarios](https://arxiv.org/abs/2012.07474)
44. [DeepSweep: An Evaluation Framework for Mitigating DNN Backdoor Attacks using Data Augmentation](https://arxiv.org/abs/2012.07006)
45. [Poison Attacks against Text Datasets with Conditional Adversarially Regularized Autoencoder](https://arxiv.org/abs/2010.02684)
46. [Strong Data Augmentation Sanitizes Poisoning and Backdoor Attacks Without an Accuracy Tradeoff](https://arxiv.org/abs/2011.09527)
47. [BaFFLe: Backdoor detection via Feedback-based Federated Learning](https://arxiv.org/abs/2011.02167)
48. [Detecting Backdoors in Neural Networks Using Novel Feature-Based Anomaly Detection](https://arxiv.org/abs/2011.02526)
49. [Mitigating Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2011.01767)
50. [FaceHack: Triggering backdoored facial recognition systems using facial characteristics](https://arxiv.org/abs/2006.11623)
51. [Customizing Triggers with Concealed Data Poisoning](https://arxiv.org/abs/2010.12563)
52. [Backdoor Learning: A Survey](https://arxiv.org/abs/2007.08745)
53. [Rethinking the Trigger of Backdoor Attack](https://arxiv.org/abs/2004.04692)
54. [AEGIS: Exposing Backdoors in Robust Machine Learning Models](https://arxiv.org/abs/2003.00865)
55. [Weight Poisoning Attacks on Pre-trained Models](https://arxiv.org/abs/2004.06660)
56. [Poisoned classifiers are not only backdoored, they are fundamentally broken](https://arxiv.org/abs/2010.09080)
57. [Input-Aware Dynamic Backdoor Attack](https://arxiv.org/abs/2010.08138)
58. [Reverse Engineering Imperceptible Backdoor Attacks on Deep Neural Networks for Detection and Training Set Cleansing](https://arxiv.org/abs/2010.07489)
59. [BAAAN: Backdoor Attacks Against Autoencoder and GAN-Based Machine Learning Models](https://arxiv.org/abs/2010.03007)
60. [Don’t Trigger Me! A Triggerless Backdoor Attack Against Deep Neural Networks](https://arxiv.org/abs/2010.03282)
61. [Toward Robustness and Privacy in Federated Learning: Experimenting with Local and Central Differential Privacy](https://arxiv.org/abs/2009.03561)
62. [CLEANN: Accelerated Trojan Shield for Embedded Neural Networks](https://arxiv.org/abs/2009.02326)
63. [Witches’ Brew: Industrial Scale Data Poisoning via Gradient Matching](https://arxiv.org/abs/2009.02276)
64. [Intrinsic Certified Robustness of Bagging against Data Poisoning Attacks](https://arxiv.org/abs/2008.04495)
65. [Can Adversarial Weight Perturbations Inject Neural Backdoors?](https://arxiv.org/abs/2008.01761)
66. [Trojaning Language Models for Fun and Profit](https://arxiv.org/abs/2008.00312)
67. [Practical Detection of Trojan Neural Networks: Data-Limited and Data-Free Cases](https://arxiv.org/abs/2007.15802)
68. [Class-Oriented Poisoning Attack](https://arxiv.org/abs/2008.00047)
69. [Noise-response Analysis for Rapid Detection of Backdoors in Deep Neural Networks](https://arxiv.org/abs/2008.00123)
70. [Cassandra: Detecting Trojaned Networks from Adversarial Perturbations](http://arxiv.org/abs/2007.14433)
71. [Backdoor Learning: A Survey](http://arxiv.org/abs/2007.08745)
72. [Backdoor Attacks and Countermeasures on Deep Learning: A Comprehensive Review](http://arxiv.org/abs/2007.10760)
73. [Live Trojan Attacks on Deep Neural Networks](http://arxiv.org/abs/2004.11370)
74. [Odyssey: Creation, Analysis and Detection of Trojan Models](https://arxiv.org/abs/2007.08142)
75. [Data Poisoning Attacks Against Federated Learning Systems](http://arxiv.org/abs/2007.08432)
76. [Blind Backdoors in Deep Learning Models](http://arxiv.org/abs/2005.03823)
77. [Deep Learning Backdoors](http://arxiv.org/abs/2007.08273)
78. [Attack of the Tails: Yes, You Really Can Backdoor Federated Learning](https://arxiv.org/abs/2007.05084)
79. [Backdoor Attacks on Facial Recognition in the Physical World](http://arxiv.org/abs/2006.14580)
80. [Graph Backdoor](http://arxiv.org/abs/2006.11890)
81. [Backdoor Attacks to Graph Neural Networks](http://arxiv.org/abs/2006.11165)
82. [You Autocomplete Me: Poisoning Vulnerabilities in Neural Code Completion](http://arxiv.org/abs/2007.02220)
83. [Reflection Backdoor: A Natural Backdoor Attack on Deep Neural Networks](http://arxiv.org/abs/2007.02343)
84. [Trembling triggers: exploring the sensitivity of backdoors in DNN-based face recognition](https://doi.org/10.1186/s13635-020-00104-z)
85. [Just How Toxic is Data Poisoning? A Unified Benchmark for Backdoor and Data Poisoning Attacks](https://arxiv.org/abs/2006.12557)
86. [Adversarial Machine Learning -- Industry Perspectives](https://arxiv.org/abs/2002.05646)
87. [ConFoc: Content-Focus Protection Against Trojan Attacks on Neural Networks](https://arxiv.org/abs/2007.00711)
88. [Model-Targeted Poisoning Attacks: Provable Convergence and Certified Bounds](https://arxiv.org/abs/2006.16469)
89. [Deep Partition Aggregation: Provable Defense against General Poisoning Attacks](https://arxiv.org/abs/2006.14768)
90. [The TrojAI Software Framework: An OpenSource tool for Embedding Trojans into Deep Learning Models*](https://arxiv.org/abs/2003.07233)
91. [Influence Function based Data Poisoning Attacks to Top-N Recommender Systems](https://arxiv.org/abs/2002.08025)
92. [BadNL: Backdoor Attacks Against NLP Models](https://arxiv.org/abs/2006.01043)
    <details>
      <summary>
      Summary
      </summary>  

      * Introduces first example of backdoor attacks against NLP models using Char-level, Word-level, and Sentence-level triggers (these different triggers operate on the level of their descriptor) 
        * Word-level trigger picks a word from the target model’s dictionary and uses it as a trigger
        * Char-level trigger uses insertion, deletion or replacement to modify a single character in a chosen word’s location (with respect to the sentence, for instance, at the start of each sentence) as the trigger.
        * Sentence-level trigger changes the grammar of the sentence and use this as the trigger
      * Authors impose an additional constraint that requires inserted triggers to not change the sentiment of text input
      * Proposed backdoor attack achieves 100% backdoor accuracy with only a drop of 0.18%, 1.26%, and 0.19% in the models utility, for the IMDB, Amazon, and Stanford Sentiment Treebank datasets
    </details>
1. [Neural Network Calculator for Designing Trojan Detectors*](https://arxiv.org/abs/2006.03707)
1. [Dynamic Backdoor Attacks Against Machine Learning Models](https://arxiv.org/abs/2003.03675)
1. [Vulnerabilities of Connectionist AI Applications: Evaluation and Defence](https://arxiv.org/abs/2003.08837)
1. [Backdoor Attacks on Federated Meta-Learning](https://arxiv.org/abs/2006.07026)
1. [Defending Support Vector Machines against Poisoning Attacks: the Hardness and Algorithm](https://arxiv.org/abs/2006.07757)
1. [Backdoors in Neural Models of Source Code](https://arxiv.org/abs/2006.06841)
1. [A new measure for overfitting and its implications for backdooring of deep learning](https://arxiv.org/abs/2006.06721)
1. [An Embarrassingly Simple Approach for Trojan Attack in Deep Neural Networks](https://arxiv.org/abs/2006.08131)
1. [MetaPoison: Practical General-purpose Clean-label Data Poisoning](https://arxiv.org/abs/2004.00225)
1. [Backdooring and Poisoning Neural Networks with Image-Scaling Attacks](https://arxiv.org/abs/2003.08633)
1. [Bullseye Polytope: A Scalable Clean-Label Poisoning Attack with Improved Transferability](https://arxiv.org/abs/2005.00191)
1. [On the Effectiveness of Mitigating Data Poisoning Attacks with Gradient Shaping](https://arxiv.org/abs/2002.11497)
1. [A Survey on Neural Trojans](https://eprint.iacr.org/2020/201.pdf)
1. [STRIP: A Defence Against Trojan Attacks on Deep Neural Networks](https://arxiv.org/abs/1902.06531)
    <details>
      <summary>
      Summary
      </summary>  

      * Authors introduce a run-time based trojan detection system called STRIP or STRong Intentional Pertubation which focuses on models in computer vision
      * STRIP works by intentionally perturbing incoming inputs (ie. by image blending) and then measuring entropy to determine whether the model is trojaned or not. Low entropy violates the input-dependance assumption for a clean model and thus indicates corruption 
      * Authors validate STRIPs efficacy on MNIST,CIFAR10, and GTSRB acheiveing false acceptance rates of below 1%
    </details>
1. [TrojDRL: Trojan Attacks on Deep Reinforcement Learning Agents](https://arxiv.org/abs/1903.06638)
1. [Demon in the Variant: Statistical Analysis of DNNs for Robust Backdoor Contamination Detection](https://arxiv.org/abs/1908.00686)
1. [Regula Sub-rosa: Latent Backdoor Attacks on Deep Neural Networks](https://arxiv.org/abs/1905.10447)
1. [Februus: Input Purification Defense Against Trojan Attacks on Deep Neural Network Systems](https://arxiv.org/abs/1908.03369)
1. [TBT: Targeted Neural Network Attack with Bit Trojan](https://arxiv.org/abs/1909.05193)
1. [Bypassing Backdoor Detection Algorithms in Deep Learning](https://arxiv.org/abs/1905.13409)
1. [A backdoor attack against LSTM-based text classification systems](https://arxiv.org/abs/1905.12457)
1. [Invisible Backdoor Attacks Against Deep Neural Networks](https://arxiv.org/abs/1909.02742)
1. [Detecting AI Trojans Using Meta Neural Analysis](https://arxiv.org/abs/1910.03137)
1. [Label-Consistent Backdoor Attacks](https://arxiv.org/abs/1912.02771)
1. [Detection of Backdoors in Trained Classifiers Without Access to the Training Set](https://arxiv.org/abs/1908.10498)
1. [ABS: Scanning neural networks for back-doors by artificial brain stimulation](https://www.cs.purdue.edu/homes/taog/docs/CCS19.pdf)
1. [NeuronInspect: Detecting Backdoors in Neural Networks via Output Explanations](https://arxiv.org/abs/1911.07399)
1. [Universal Litmus Patterns: Revealing Backdoor Attacks in CNNs](https://arxiv.org/abs/1906.10842)
1. [Programmable Neural Network Trojan for Pre-Trained Feature Extractor](https://arxiv.org/abs/1901.07766)
1. [Demon in the Variant: Statistical Analysis of DNNs for Robust Backdoor Contamination Detection](https://arxiv.org/abs/1908.00686)
1. [TamperNN: Efficient Tampering Detection of Deployed Neural Nets](https://arxiv.org/abs/1903.00317)
1. [TABOR: A Highly Accurate Approach to Inspecting and Restoring Trojan Backdoors in AI Systems](https://arxiv.org/abs/1908.01763)
1. [Design of intentional backdoors in sequential models](https://arxiv.org/abs/1902.09972)
1. [Design and Evaluation of a Multi-Domain Trojan Detection Method on ins Neural Networks](https://arxiv.org/abs/1911.10312)
1. [Poison as a Cure: Detecting &amp; Neutralizing Variable-Sized Backdoor Attacks in Deep Neural Networks](https://arxiv.org/abs/1911.08040)
1. [Data Poisoning Attacks on Stochastic Bandits](https://arxiv.org/abs/1905.06494)
1. [Hidden Trigger Backdoor Attacks](https://arxiv.org/abs/1910.00033)
1. [Deep Poisoning Functions: Towards Robust Privacy-safe Image Data Sharing](https://arxiv.org/abs/1912.06895)
1. [A new Backdoor Attack in CNNs by training set corruption without label poisoning](https://arxiv.org/abs/1902.11237)
1. [Deep k-NN Defense against Clean-label Data Poisoning Attacks](https://arxiv.org/abs/1909.13374)
1. [Transferable Clean-Label Poisoning Attacks on Deep Neural Nets](https://arxiv.org/abs/1905.05897)
1. [Revealing Backdoors, Post-Training, in DNN Classifiers via Novel Inference on Optimized Perturbations Inducing Group Misclassification](https://arxiv.org/abs/1908.10498)
1. [Explaining Vulnerabilities to Adversarial Machine Learning through Visual Analytics](https://arxiv.org/abs/1907.07296)
1. [Subpopulation Data Poisoning Attacks](https://www.ccis.northeastern.edu/home/jagielski/subpop_finance.pdf)
1. [TensorClog: An imperceptible poisoning attack on deep neural network applications](https://ieeexplore.ieee.org/document/8668758)
1. [DeepInspect: A black-box trojan detection and mitigation framework for deep neural networks](https://cseweb.ucsd.edu/~jzhao/files/DeepInspect-IJCAI2019.pdf)
1. [Resilience of Pruned Neural Network Against Poisoning Attack](https://ieeexplore.ieee.org/document/8659362)
1. [Spectrum Data Poisoning with Adversarial Deep Learning](https://arxiv.org/abs/1901.09247)
1. [Neural cleanse: Identifying and mitigating backdoor attacks in neural networks](https://people.cs.uchicago.edu/~huiyingli/publication/backdoor-sp19.pdf)
1. [SentiNet: Detecting Localized Universal Attacks Against Deep Learning Systems](https://arxiv.org/abs/1812.00292)
    <details>
      <summary>
      Summary
      </summary>  

      * Authors develop SentiNet detection framework for locating universal attacks on neural networks
      * SentiNet is ambivalent to the attack vectors and uses model visualization / object detection techniques to extract potential attacks regions from the models input images.  The potential attacks regions are identified as being the parts that influence the prediction the most. After extraction, SentiNet applies these regions to benign inputs and uses the original model to analyze the output 
      * Authors stress test the SentiNet framework on three different types of attacks— data poisoning attacks, Trojan attacks, and adversarial patches. They are able to show that the framework achieves competitive metrics across all of the attacks  (average true positive rate of 96.22% and an average true negative rate of 95.36%) 
    </details>
1. [PoTrojan: powerful neural-level trojan designs in deep learning models](https://arxiv.org/abs/1802.03043)
1. [Hardware Trojan Attacks on Neural Networks](https://arxiv.org/abs/1806.05768)
1. [Spectral Signatures in Backdoor Attacks](https://arxiv.org/abs/1811.00636)
    <details>
      <summary>
      Summary
      </summary>  

      * Identified a  "spectral signatures" property of current backdoor attacks which allows the authors to use robust statistics to stop Trojan attacks 
      * The "spectral signature" refers to a change in the covariance spectrum of learned feature representations that is left after a network is attacked. This can be detected by using singular value decomposition (SVD). SVD is used to identify which examples to remove from the training set. After these examples are removed the model is retrained on the cleaned dataset and is no longer Trojaned. The authors test this method on the CIFAR 10 image dataset.
    </details>
1. [Defending Neural Backdoors via Generative Distribution Modeling](https://arxiv.org/abs/1910.04749)
1. [Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering](https://arxiv.org/abs/1811.03728)
    <details>
      <summary>
      Summary
      </summary>  

      * Proposes Activation Clustering approach to backdoor detection/ removal which analyzes the neural network activations for anomalies and works for both text and images
      * Activation Clustering uses dimensionality techniques (ICA, PCA) on the activations and then clusters them using k-means (k=2) along with a silhouette score metric to separate poisoned from clean clusters  
      * Shows that Activation Clustering is successful on three different image/datasets (MNIST, LISA, Rotten Tomatoes)  as well as in settings where multiple Trojans are inserted and classes are multi-modal 
    </details>
1. [Model-Reuse Attacks on Deep Learning Systems](https://arxiv.org/abs/1812.00483)
1. [How To Backdoor Federated Learning](https://arxiv.org/abs/1807.00459)
1. [Trojaning Attack on Neural Networks](https://docs.lib.purdue.edu/cgi/viewcontent.cgi?article=2782&context=cstech)
1. [Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks](https://arxiv.org/abs/1804.00792)
    <details>
      <summary>
      Summary
      </summary>  

      * Proposes neural network poisoning attack that uses "clean labels" which do not require the adversary to mislabel training inputs
      * The paper also presents a optimization based method for generating their poisoning attacks and provides a watermarking strategy for end-to-end attacks that improves the poisoning reliability 
      * Authors demonstrate their method by using generated poisoned frog images from the CIFAR
dataset to manipulate different kinds of image classifiers
    </details>
1. [Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks](https://arxiv.org/abs/1805.12185)
    <details>
      <summary>
      Summary
      </summary>  

      * Investigate two potential detection methods for backdoor attacks  (Fine-tuning and pruning). They find both are insufficient on their own and thus propose a combined detection method which they call "Fine-Pruning"  
      * Authors go on to show that on three backdoor techniques "Fine-Pruning" is able to eliminate or reduce Trojans on datasets in the traffic sign, speech, and face recognition domains  
    </details>
1. [Technical Report: When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks](https://arxiv.org/abs/1803.06975)
1. [Backdoor Embedding in Convolutional Neural Network Models via Invisible Perturbation](https://arxiv.org/abs/1808.10307)
1. [Hu-Fu: Hardware and Software Collaborative Attack Framework against Neural Networks](https://arxiv.org/abs/1805.05098)
1. [Attack Strength vs. Detectability Dilemma in Adversarial Machine Learning](https://arxiv.org/abs/1802.07295)
1. [Data Poisoning Attacks in Contextual Bandits](https://arxiv.org/abs/1808.05760)
1. [BEBP: An Poisoning Method Against Machine Learning Based IDSs](https://arxiv.org/abs/1803.03965)
1. [Generative Poisoning Attack Method Against Neural Networks](https://arxiv.org/abs/1703.01340)
1. [BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain](https://arxiv.org/abs/1708.06733)
    <details>
      <summary>
      Summary
      </summary>  

      * Introduce Trojan Attacks— a type of attack where an adversary can create a maliciously trained network (a backdoored neural network, or a BadNet) that has state-of-the-art performance on the user’s training and validation samples, but behaves badly on specific attacker-chosen inputs
      * Demonstrate backdoors in a more realistic scenario by creating a U.S. street sign classifier that identifies stop signs as speed limits when a special sticker is added to the stop sign

    </details>
1. [Towards Poisoning of Deep Learning Algorithms with Back-gradient Optimization](https://arxiv.org/abs/1708.08689)
1. [Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning](https://arxiv.org/abs/1712.05526)
1. [Neural Trojans](https://arxiv.org/abs/1710.00942)
1. [Towards Poisoning of Deep Learning Algorithms with Back-gradient Optimization](https://arxiv.org/abs/1708.08689)
1. [Certified defenses for data poisoning attacks](https://arxiv.org/abs/1706.03691)
1. [Data Poisoning Attacks on Factorization-Based Collaborative Filtering](https://arxiv.org/abs/1608.08182)
1. [Data poisoning attacks against autoregressive models](https://dl.acm.org/doi/10.5555/3016100.3016102)
1. [Using machine teaching to identify optimal training-set attacks on machine learners](https://dl.acm.org/doi/10.5555/2886521.2886721)
1. [Poisoning Attacks against Support Vector Machines](https://arxiv.org/abs/1206.6389)
1. [Backdoor Attacks against Learning Systems](https://par.nsf.gov/servlets/purl/10066467)
1. [Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning](https://arxiv.org/abs/1712.05526)
1. [Antidote: Understanding and defending against poisoning of anomaly detectors](https://dl.acm.org/doi/10.1145/1644893.1644895)
